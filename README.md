# Top K frequent data + log data analytics

PA1: Word Count (**Python**)
In this assignment, we are going to find the top k frequent word in the big dataset, which is 300MB, 2.5GB and 16GB. We present an analysis of the performance of our implementation of a top-K most frequent words algorithm using different input dataset sizes and optimization techniques. We evaluate the algorithm's efficiency in terms of execution time, memory usage, and CPU utilization, and we analyze the impact of different algorithms, data structures, and system resources on its performance. 

## How to use it
1. Run "topkwords.py"

2. input the filepath and stopword path

3. choose how many words you would like to see (k)

4. set the sharding number

5. set the num_processes(default: 4)



PA2: Word Count (**Python + Hadoop MapReduce**)

PA3: Word Count (**Apache Spark**) + NASA log data analytics(**PySpark + Kafka + DBFS(Databricks) + HDFS**)

**All include code and project analysis report
